<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="Aligning text and images boosts performance when using diffusion-based perception models.">
    <meta property="og:title" content="Text-image Alignment for Diffusion-based Perception (TADP)"/>
    <meta property="og:description" content="Aligning text and images boosts diffusion-based perception performance."/>
    <meta property="og:url" content="http://www.vision.caltech.edu/tadp/"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="http://www.vision.caltech.edu/tadp/static/images/tadp_fig1_v2_wide_resized.png"/>
    <meta property="og:image:width" content="1083"/>
    <meta property="og:image:height" content="600"/>


    <meta name="twitter:title" content="Text-image Alignment for Diffusion-based Perception (TADP)">
    <meta name="twitter:description" content="Aligning text and images boosts performance when using diffusion-based perception models">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="http://www.vision.caltech.edu/tadp/static/images/tadp_fig1_v2_wide_resized.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="text, image, alignment, CLIP, diffusion, Stable Diffusion, perception, computer vision,
    VLM, vision-language models, LLVM, large language and vision models, object-detection, segmentation, depth estimation,
    NYU_v2, ADE20K, Pascal, Pascal Watercolor, Pascal Comic, Cityscapes, Dark Zurich, Night Driving.">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>TADP</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>

    <style>
        /* Style for the container that holds the images */
        .image-container {
            display: flex; /* Use flexbox to place images side by side */
            justify-content: space-between; /* Space between the images */
        }

        /* Style for individual images */
        .image-container img {
            width: 90%; /* Adjust the width as needed */
            max-width: 100%; /* Ensure images don't exceed their original size */
        }
    </style>
</head>
<body>



<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Text-Image Alignment for
                        Diffusion-Based Perception</h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block" id="first-author-list">
                            <custom-href href="https://nkondapa.github.io/" target="_blank">Neehar Kondapaneni</custom-href>
                            <custom-href href="https://scholar.google.com/citations?user=XkVkcywAAAAJ&hl=en" target="_blank">Markus Marks</custom-href><,
                            <custom-href href="https://scholar.google.com/citations?user=e9xfiKEAAAAJ&hl=en&oi=ao" target="_blank">Manuel Knott</custom-href>,
                        </span>
                        <br>
                        <span class="author-block">
                            <a href="https://rogeriojr.com/" target="_blank">Rog&eacute;rio Guimar&atilde;es</a>,
                            <a href="https://www.vision.caltech.edu/" target="_blank">Pietro Perona</a>
                        </span>

                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"> California Institute of Technology, ETH Zurich</span>
                        <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- Arxiv PDF link -->
                            <span class="link-block">
                        <a href="https://arxiv.org/pdf/2310.00031.pdf" target="_blank"
                           class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                            <!--                    &lt;!&ndash; Supplementary PDF link &ndash;&gt;-->
                            <!--                    <span class="link-block">-->
                            <!--                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"-->
                            <!--                      class="external-link button is-normal is-rounded is-dark">-->
                            <!--                      <span class="icon">-->
                            <!--                        <i class="fas fa-file-pdf"></i>-->
                            <!--                      </span>-->
                            <!--                      <span>Supplementary</span>-->
                            <!--                    </a>-->
                            <!--                  </span>-->

                            <!-- Github link -->
<!--                            <span class="link-block">-->
<!--                    <a href="https://github.com/damaggu/TADP" target="_blank"-->
<!--                       class="external-link button is-normal is-rounded is-dark">-->
<!--                    <span class="icon">-->
<!--                      <i class="fab fa-github"></i>-->
<!--                    </span>-->
<!--                    <span>Code</span>-->
<!--                  </a>-->
<!--                </span>-->

                            <!-- ArXiv abstract Link -->
                            <span class="link-block">
                  <a href="https://arxiv.org/abs/2310.00031" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<script>
    // Define a custom HTML element
    class CustomHref extends HTMLElement {
        constructor() {
            super();

            // Create a superscript element
            const sup = document.createElement('sup');
            sup.textContent = '*';

            // Create a link element
            const link = document.createElement('a');
            link.href = this.getAttribute('href');
            link.textContent = this.textContent;
            // Create a comma element
            const comma = document.createElement('span');
            comma.textContent = ', ';

            // Append the link and superscript to the custom element
            this.textContent = '';
            this.appendChild(link);
            this.appendChild(sup);
            this.appendChild(comma);
        }
    }

    // Register the custom element with the browser
    customElements.define('custom-href', CustomHref);
</script>

<script>

    // Function to shuffle an array using Fisher-Yates algorithm
    function shuffleArray(array) {
        for (let i = array.length - 1; i > 0; i--) {
            const j = Math.floor(Math.random() * (i + 1));
            [array[i], array[j]] = [array[j], array[i]];
        }
    }
    // Function to shift the order of an array one position to the right
    function shiftArrayRight(array) {
        const lastItem = array.pop();
        array.unshift(lastItem);
    }

    // Function to refresh the author list order based on the counter
    function refreshAuthorList() {
        const authorList = document.getElementById('first-author-list');
        const authors = Array.from(authorList.children);

        // Shift the order of authors one position to the right
        shiftArrayRight(authors);

        authorList.innerHTML = '';
        authors.forEach(author => {
            authorList.appendChild(author);
        });

    }

    // Initial randomization of the author list
    const authorsArray = Array.from(document.getElementById('first-author-list').children);
    shuffleArray(authorsArray);

    // Replace the original author list with the shuffled one
    const authorList = document.getElementById('first-author-list');
    authorList.innerHTML = '';
    authorsArray.forEach(author => {
        authorList.appendChild(author);
    });

    // Set an interval to refresh the author list every 10 seconds (10000 milliseconds)
    setInterval(refreshAuthorList, 7000);
</script>



<!--&lt;!&ndash; Teaser video&ndash;&gt;-->
<!--<section class="hero teaser">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <video poster="" id="tree" autoplay controls muted loop height="100%">-->
<!--        &lt;!&ndash; Your video here &ndash;&gt;-->
<!--        <source src="static/videos/banner_video.mp4"-->
<!--        type="video/mp4">-->
<!--      </video>-->
<!--      <h2 class="subtitle has-text-centered">-->
<!--        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. -->
<!--      </h2>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--&lt;!&ndash; End teaser video &ndash;&gt;-->

<!-- Overview Image -->
<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <div class="hero-body">
                    <!-- Your image here -->
                    <img src="static/images/tadp_fig1_v3.png" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-justified">
                        <b>Text-image/domain alignment for guided diffusion perception.</b> We present a framework called
                        <b>T</b>ext-<b>A</b>ligned <b>D</b>iffusion <b>P</b>erception (TADP) to use image captions to guide a
                        diffusion-pretrained vision model for tasks like depth estimation, semantic segmentation, and object
                        detection. For cross-domain settings, we demonstrate how to modify captions to improve performance on
                        the
                        target domain. We study the relationship between text-image alignment and vision task outcome for each
                        domain.
                    </h2>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-full">
                <h2 class="title is-3">Abstract</h2>
            </div>
        </div>
    </div>
</section>
<!-- End paper abstract -->

<section class="section hero is-small">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <div class="content has-text-justified">
                    <p>
                        Pre-trained diffusion models have been used to boost accuracy in visual perception tasks, such
                        as semantic
                        segmentation and monocular depth estimation &mdash; the attention maps generated by the
                        diffusion model play
                        a pivotal role in enhancing the efficacy of task-specific heads.<br><br>

                        What role does the diffusion model's prompt play? What is the best way to optimize it? We
                        investigate these
                        questions and find that automatically generated captions can improve text-image alignment and
                        significantly
                        enhance a model's cross-attention maps, leading to better perceptual performance. <b>Our approach
                        improves upon
                        the current SOTA in diffusion-based semantic segmentation for ADE20K, improving 1.7 mIoU (+3.2%)
                        and the
                        current overall SOTA in depth estimation by 0.2 RMSE (+8%) on NYUv2.</b><br><br>

                        Additionally, we find that captions help in cross-domain adaptation when we align text prompts
                        to the target
                        domain. <b>Our object detection method, trained on Pascal VOC2012, achieves SOTA results on
                        Watercolor2K and
                        Comic2K. Our segmentation method, trained on Cityscapes, achieves SOTA results on the Dark
                        Zurich and
                        Nighttime Driving benchmarks.</b>
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>


<!-- Paper Method -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-full">
                <h2 class="title is-3">Methods</h2>
            </div>
        </div>
    </div>
</section>
<!-- End Method -->

<!--  Single domain method-->
<section class="section hero is-small">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-4">Single-domain</h2>
                <img src="static/images/methods.gif" alt="MY ALT TEXT"/>
                <div class="content has-text-justified">
                    <p><b>Overview of our single-domain method.</b> Our method makes use of the stable diffusion backbone.
                        We encode an image into its latent representation and pass it through a single step of the
                        diffusion model. We concatenate the cross-attention maps, the multi-scale
                        feature maps, and the latent code for the image (not shown), before passing them to the vision
                        specific decoder. In the blue box, we show various captioning/prompting strategies with
                        differing levels
                        of text-image alignment. We find that increasing alignment significantly improves performance.
                        Our best performing method (excluding an Oracle method) uses BLIP-2 to caption the image and
                        feeds the caption to the backbone as the prompt.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<!--  Cross-domain Method-->
<section class="section hero is-small">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-4">Cross-domain</h2>
                <img src="static/images/domain-alignment.gif" alt="MY ALT TEXT"/>
                <div class="content has-text-justified">
                    <p><b>Overview of our cross-domain method.</b> Starting from our best single-domain alignment method
                      (captioning with BLIP-2), we modify the caption to improve alignment with the target domain. We
                      try several methods, from simple text to more powerful model personalization strategies. Model
                      personalization strategies take a random sample of images and learn to generate images that are similar.
                      We find that personalizing the model to the target domain as a pre-training step (with unlabeled target domain images)
                      improves generalization to the target domain.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-full">
                <h2 class="title is-3">Analysis</h2>
            </div>
        </div>
    </div>
</section>
<!-- End paper abstract -->


<!--  Analysis-->
<section class="section hero is-small">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-4">Single-domain</h2>
                <div class="content has-text-justified">
                    <p> We analyze the effects of different prompting strategies. We qualitatively explore the effect of
                        off-target classes using an image-to-image variation pipeline on a pretrained Stable Diffusion backbone.
                        Then, we quantitatively measure the effect of off-target classes on our Oracle model.
                        We find that off-target classes have a
                        significant effect on the Oracle model, and that the effect is picked up by the task-specific
                        head.
                    </p>
                </div>
                <div class="image-container">
                    <div>
                        <h2 class="title is-4">Class Names</h2>
                        <img src="static/images/img2img_class_names.gif" alt="MY ALT TEXT"/>
                    </div>
                    <div>
                        <h2 class="title is-4">BLIP Captions</h2>
                        <img src="static/images/img2img_blip.gif" alt="MY ALT TEXT"/>
                    </div>
                </div>
                <br>
                <div class="content has-text-justified">
                    <p><b>Qualitative image-to-image variation analysis.</b>
                        We use the image to image (img2img) variation pipeline (with Stable Diffusion 1.5 weights) to
                        qualitatively analyze the effects of prompts with off-target classes. In the gif, we generate an
                        image variation of the original image at differing strength ratios. On the left we show the Class
                        Names prompt, which is simply a string of class names. On the right we use an aligned BLIP prompt.
                        The effect of off-target classes is most clear in stronger variations in which objects belonging
                        to off-target classes (a potted plant or a sofa) become more prominent.
                        These qualitative results imply that this kind of
                        prompt modifies the latent representation to incorporate information about off-target classes,
                        potentially making the downstream task more difficult.
                        In contrast, using the BLIP prompt changes the image, but the semantics
                        (position of objects, classes present) of the image variation are significantly closer to the
                        original.
                        These results suggest a mechanism for how off-target classes may impact our vision models. In
                        the next analysis, we quantitatively measure this effect using a fully trained Oracle model.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<!--  Analysis-->
<section class="section hero is-small">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <!--        <h2 class="title is-4">Architecture</h2>-->
                <img src="static/images/matrix.gif" alt="MY ALT TEXT"/>
                <div class="content has-text-justified">
                    <p><b>Quantitative effect of Class Names on Oracle model.</b> To quantify the impact of the
                        off-target classes
                        on the downstream vision task, we measure the averaged pixel-wise scores (normalized via
                        Softmax) per class
                        when passing the class names to the Oracle segmentation model for Pascal VOC 2012. We compare
                        this to the
                        original oracle prompt. We find that including the off-target prompts significantly increases
                        the
                        probability of a pixel being misclassified as one of the semantically nearby off-target classes.
                        For
                        example, if the original image contains a cow, including the words dog and horse
                        significantly raises
                        the probability of mis-classifying the pixels belonging to the cow as pixels belonging to a dog
                        or a horse.
                        These results indicate that the effect of off-target classes is picked up by the task-specific
                        head and is
                        incorporated into the output.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<!--&lt;!&ndash; Paper abstract &ndash;&gt;-->
<!--<section class="section hero is-light">-->
<!--    <div class="container is-max-desktop">-->
<!--        <div class="columns is-centered has-text-centered">-->
<!--            <div class="column is-full">-->
<!--                <h2 class="title is-3">Cross-Domain</h2>-->
<!--            </div>-->
<!--        </div>-->
<!--    </div>-->
<!--</section>-->
<!--&lt;!&ndash; End paper abstract &ndash;&gt;-->

<!--  Methods-->
<section class="section hero is-small">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                            <h2 class="title is-4">Cross-domain</h2>
                <br>

<!--                <div class="image-container">-->
<!--                    <div>-->
<!--&lt;!&ndash;                        <h2 class="title is-4">Class Names</h2>&ndash;&gt;-->
<!--                        <img src="static/images/cityscapes_dreambooth.png" alt="MY ALT TEXT"/>-->
<!--                    </div>-->
<!--                    <div>-->
<!--&lt;!&ndash;                        <h2 class="title is-4">BLIP Captions</h2>&ndash;&gt;-->
<!--                        <img src="static/images/pascal_dreambooth.png" alt="MY ALT TEXT"/>-->
<!--                    </div>-->
<!--                </div>-->
                <div class="content has-text-justified">
                    <p><b>Qualitative examples of Dreambooth and Textual Inversionfor Cross-Domain</b> We show examples of images generated
                        with Dreambooth and Textual Inversion in target datasets: Dark Zurich, Comic2k, Watercolor2k.
                        They are qualitatively similar to images from the real datasets, indicating that the learned tokens are able
                        to capture the style of the target domains. These tokens (and thie backbone for DreamBooth) are used in our alignment
                        strategy for the cross-domain experiments.
                    </p>
                </div>
                <img src="static/images/TI_Tokens_DZ.jpg" alt="MY ALT TEXT"/>
                Textual inversion and Dreambooth tokens of Cityscapes to Dark Zurich<br><br>
                <img src="static/images/TI_Tokens_comic.jpg" alt="MY ALT TEXT"/>
                Textual inversion and Dreambooth tokens of VOC to Comic<br><br>
                <img src="static/images/TI_Tokens_watercolor.jpg" alt="MY ALT TEXT"/>
                Textual inversion and Dreambooth tokens of VOC to Watercolor2k

            </div>
        </div>
    </div>
</section>

<!--&lt;!&ndash;Results&ndash;&gt;-->
<!--<section class="section hero is-small">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <h2 class="title is-4">Semantic Segmentation</h2>-->
<!--        <img src="static/images/tables/table-1.png" alt="MY ALT TEXT"/>-->
<!--        <div class="content has-text-justified">-->
<!--          <p><b>(a) Prompting for Pascal VOC2012 Segmentation.</b> We report the single scale validation mIoU for-->
<!--            Pascal experiments. Avg: EOS token averaging, TA: Text Adapter, LS: Latent Scaling, G: Grammar, OT:-->
<!--            Off-target information. For our method, we indicate the minimum length of the BLIP caption with TADP-X-->
<!--            and nouns only with (NO). <b>(b) Semantic segmentation with different methods for ADE20k.</b> Our method-->
<!--            (green) achieves SOTA within the diffusion-pretrained models category. The results of our oracle indicate-->
<!--            the potential of diffusion-based models for future research as it is significantly higher than the overall-->
<!--            SOTA (highlighted in yellow).-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->

<!--<section class="section hero is-small">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <h2 class="title is-4">Depth</h2>-->
<!--        <img src="static/images/tables/table-2.png" alt="MY ALT TEXT"/>-->
<!--        <div class="content has-text-justified">-->
<!--          <p><b>Depth estimation in NYUv2.</b> We find that for depth estimation, latent scaling is particularly-->
<!--            important, accounting for a relative gain of ~5.5% on the RMSE metric. Improved image-text alignment-->
<!--            also has a significant effect, improving ~4% relative on the RMSE metric. We find that the longest-->
<!--            BLIP captions we try, 40 tokens, perform the best. The same effects hold on the fast schedule, but are less-->
<!--            pronounced.-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->

<!--<section class="section hero is-small">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <h2 class="title is-4">Cross-Domain</h2>-->
<!--        <img src="static/images/tables/table-3.png" alt="MY ALT TEXT"/>-->
<!--        <div class="content has-text-justified">-->
<!--          <p><b>Cross-domain perception results.</b> Cityscapes to Dark Zurich and Nighttime Driving (ND). We report-->
<!--            the mIoU. Pascal VOC2012 to Watercolor2k and Comic2k. We report the mAP and AP@0.5. Our method sets a new-->
<!--            SOTA for all these tasks.-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->

<!--&lt;!&ndash; Image carousel &ndash;&gt;-->
<!--<section class="hero is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <div id="results-carousel" class="carousel results-carousel">-->
<!--       <div class="item">-->
<!--        &lt;!&ndash; Your image here &ndash;&gt;-->
<!--        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>-->
<!--        <h2 class="subtitle has-text-centered">-->
<!--          First image description.-->
<!--        </h2>-->
<!--      </div>-->
<!--      <div class="item">-->
<!--        &lt;!&ndash; Your image here &ndash;&gt;-->
<!--        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>-->
<!--        <h2 class="subtitle has-text-centered">-->
<!--          Second image description.-->
<!--        </h2>-->
<!--      </div>-->
<!--      <div class="item">-->
<!--        &lt;!&ndash; Your image here &ndash;&gt;-->
<!--        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>-->
<!--        <h2 class="subtitle has-text-centered">-->
<!--         Third image description.-->
<!--       </h2>-->
<!--     </div>-->
<!--     <div class="item">-->
<!--      &lt;!&ndash; Your image here &ndash;&gt;-->
<!--      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>-->
<!--      <h2 class="subtitle has-text-centered">-->
<!--        Fourth image description.-->
<!--      </h2>-->
<!--    </div>-->
<!--  </div>-->
<!--</div>-->
<!--</div>-->
<!--</section>-->
<!--&lt;!&ndash; End image carousel &ndash;&gt;-->


<!--&lt;!&ndash; Youtube video &ndash;&gt;-->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      &lt;!&ndash; Paper video. &ndash;&gt;-->
<!--      <h2 class="title is-3">Video Presentation</h2>-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--        <div class="column is-four-fifths">-->
<!--          -->
<!--          <div class="publication-video">-->
<!--            &lt;!&ndash; Youtube embed code here &ndash;&gt;-->
<!--            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--&lt;!&ndash; End youtube video &ndash;&gt;-->


<!--&lt;!&ndash; Video carousel &ndash;&gt;-->
<!--<section class="hero is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title is-3">Another Carousel</h2>-->
<!--      <div id="results-carousel" class="carousel results-carousel">-->
<!--        <div class="item item-video1">-->
<!--          <video poster="" id="video1" autoplay controls muted loop height="100%">-->
<!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
<!--            <source src="static/videos/carousel1.mp4"-->
<!--            type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-video2">-->
<!--          <video poster="" id="video2" autoplay controls muted loop height="100%">-->
<!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
<!--            <source src="static/videos/carousel2.mp4"-->
<!--            type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-video3">-->
<!--          <video poster="" id="video3" autoplay controls muted loop height="100%">\-->
<!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
<!--            <source src="static/videos/carousel3.mp4"-->
<!--            type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--&lt;!&ndash; End video carousel &ndash;&gt;-->


<!--&lt;!&ndash; Paper poster &ndash;&gt;-->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title">Poster</h2>-->

<!--      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">-->
<!--          </iframe>-->
<!--        -->
<!--      </div>-->
<!--    </div>-->
<!--  </section>-->
<!--&lt;!&ndash;End paper poster &ndash;&gt;-->


<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
@misc{kondapaneni2023textimage,
      title = {Text-image Alignment for Diffusion-based Perception},
	  journal = {arXiv preprint arXiv:2310.00031},
	  author = {Kondapaneni, Neehar and Marks, Markus and Knott, Manuel and Guimarães, Rogério and Perona, Pietro},
	  year = {2023}
}
        </code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">

                    <p>
                        This page was built using the <a
                            href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic
                        Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io"
                                                                                target="_blank">Nerfies</a> project
                        page. This website is licensed under a <a rel="license"
                                                                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                                                                  target="_blank">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>

                </div>
            </div>
        </div>
    </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>
</html>
